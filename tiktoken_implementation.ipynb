{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2692674, 50257)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funtion to load the data from the ipnput.txt file into a string\n",
    "def load_data():\n",
    "    with open(\"input.txt\", \"r\", encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    return data\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens, encoding.n_vocab\n",
    "\n",
    "text=load_data()\n",
    "num_tokens_from_string(text, \"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text:str):\n",
    "    return enc.encode(text)\n",
    "\n",
    "def decode(tokens:np.array):\n",
    "    return enc.decode(tokens)\n",
    "\n",
    "vocab_size = enc.n_vocab\n",
    "\n",
    "initial_sequence = '[Cartman] Carrots are good for eyesight, but so are other vegetables'\n",
    "intial_tokens = encode(initial_sequence)\n",
    "intial_tokens\n",
    "len(intial_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crappy GPT implementaiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b300fda210>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "block_size = 16\n",
    "max_iters = 300\n",
    "eval_interval = 10\n",
    "learning_rate = 8e-4\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "eval_iters = 10\n",
    "n_embed = 192\n",
    "num_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "torch.manual_seed(69) # nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and splitting of the data\n",
    "data = torch.tensor(encode(text), dtype=torch.long) # Encoding of the entire text, sotring it in a torch tensor\n",
    "n = int(0.9*len(data)) # Number of characters to use for training\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Deffinitions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # Generate a batch of data from input x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # Starting index of each sequence\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses[k].mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    # This is one head of self-attention\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size,  bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)     # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-1,-2) * C**-0.5 # (B,T,T)\n",
    "        wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf')) # Mask out the upper triangular part\n",
    "        wei = F.softmax(wei, dim=-1) # (B,T,T)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # perform the weighted aggrefation of the values\n",
    "        v  = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_head, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_head)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Feed forward\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed, num_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // num_head\n",
    "        self.sa = MultiHeadAttention(num_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Bigram module\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embed, num_head=num_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embed)  # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C) apply all blocks of heads\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        else:\n",
    "            # Reshape BCT for Pythorch's cross entropy loss\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(-1)\n",
    "            # Compute the loss entropy\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is a(B,T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:] # (B,T)\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]\n",
    "            # Apply softmax\n",
    "            probs = F.softmax(logits, dim=-1) # (B,C)\n",
    "            # Sample from the distribution\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # (B,1)\n",
    "            # Append to the context\n",
    "            idx = torch.cat([idx, next_token], dim=1) # (B,T+1)\n",
    "        return idx\n",
    "\n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in millons: 22.018129\n"
     ]
    }
   ],
   "source": [
    "# Print the number of parameters\n",
    "print(f\"Number of parameters in millons: {sum(p.numel() for p in model.parameters())/1e6}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizer\n",
      "Starting training\n",
      "step 0, train loss 10.998212, val loss 10.993601, mean time per step 1.09s\n",
      "step 10, train loss 8.638666, val loss 8.492457, mean time per step 0.98s\n",
      "step 20, train loss 6.746989, val loss 7.020180, mean time per step 0.96s\n",
      "step 30, train loss 6.519319, val loss 6.502742, mean time per step 0.96s\n",
      "step 40, train loss 6.149874, val loss 6.224807, mean time per step 0.95s\n",
      "step 50, train loss 6.233770, val loss 6.216971, mean time per step 0.95s\n",
      "step 60, train loss 6.130749, val loss 5.892414, mean time per step 0.96s\n",
      "step 70, train loss 5.838697, val loss 5.863874, mean time per step 0.96s\n",
      "step 80, train loss 5.457157, val loss 5.777126, mean time per step 0.96s\n"
     ]
    }
   ],
   "source": [
    "# Create a pytorch optimizer\n",
    "print(\"Creating optimizer\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training\")\n",
    "start_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}, train loss {losses['train']:4f}, val loss {losses['val']:4f}, mean time per step {(time.time() - start_time)/(iter+1):.2f}s\")\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Print the time taken\n",
    "print(f\"Training time: {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Generate from the model\n",
    "# context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "context = torch.tensor(intial_tokens, dtype=torch.long, device=device).reshape(-1,1)\n",
    "\n",
    "# Print all intial hyperparameters\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Block size: {block_size}\")\n",
    "print(f\"Max iterations: {max_iters}\")\n",
    "print(f\"Evaluation interval: {eval_interval}\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of embeddings: {n_embed}\")\n",
    "print(f\"Number of heads: {num_head}\")\n",
    "print(f\"Number of layers: {n_layer}\")\n",
    "print(f\"Dropout: {dropout}\")\n",
    "print(f\"Number of parameters in millons: {sum(p.numel() for p in model.parameters())/1e6}\")\n",
    "print(\"Intial sequence: \", initial_sequence)\n",
    "\n",
    "print(decode(model.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
